{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import import_ipynb\n",
    "from Preprocessing import training_sequence, LENGTH, DICT_PATH\n",
    "# from ipynb.fs.full.Preprocessing import training_sequence, LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_UNITS = 38 # Total number of unique symbols\n",
    "NUM_UNITS = [256] # Number of units in the internal layer and this case there is only one layer\n",
    "LOSS = \"sparse_categorical_crossentropy\" # Loss function\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 25 #50\n",
    "BATCH_SIZE = 64 # Amount of samples that the network is gonna see before running back propogation\n",
    "SAVE_NETWORK_PATH = \"misc/Network_model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(output_units, num_units, loss, learning_rate):\n",
    "    \n",
    "    # Model architecture\n",
    "    input_layer = keras.layers.Input(shape=(None, output_units))\n",
    "    x = keras.layers.LSTM(num_units[0])(input_layer)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    output_layer = keras.layers.Dense(output_units, activation = \"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(input_layer, output_layer)\n",
    "    \n",
    "    # Compiling Model\n",
    "    model.compile(loss=loss, \n",
    "                optimizer=keras.optimizers.Adam(lr = learning_rate), \n",
    "                metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ouput_units = OUTPUT_UNITS, num_units = NUM_UNITS, loss = LOSS, learning_rate = LEARNING_RATE):\n",
    "     # Generating training sequences\n",
    "     inputs, targets =  training_sequence(LENGTH)\n",
    "\n",
    "     # Neural Network\n",
    "     network = build_network(ouput_units, num_units, loss, learning_rate)\n",
    "\n",
    "\n",
    "     # Training the network\n",
    "     network.fit(inputs, targets, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "     network.save(SAVE_NETWORK_PATH)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dual_network(output_units, num_units, learning_rate):\n",
    "    input = keras.layers.Input(shape=(None, output_units))\n",
    "\n",
    "    bidirectional_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(input)\n",
    "    forward_lstm = tf.keras.layers.LSTM(64, return_sequences=True)(bidirectional_lstm)\n",
    "\n",
    "    concatenation = tf.keras.layers.Concatenate()([bidirectional_lstm, forward_lstm])\n",
    "\n",
    "    output = tf.keras.layers.Dense(1)(concatenation)\n",
    "\n",
    "    model = tf.keras.Model(input, output)\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(), \n",
    "                optimizer=keras.optimizers.Adam(lr = learning_rate), \n",
    "                metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model(output_units, learning_rate):\n",
    "    input = keras.layers.Input(shape=(None, output_units))\n",
    "\n",
    "    bidirectional_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(input)\n",
    "    forward_lstm = tf.keras.layers.LSTM(64, return_sequences=True)(bidirectional_lstm)\n",
    "\n",
    "    concatenation = tf.keras.layers.Concatenate()([bidirectional_lstm, forward_lstm])\n",
    "\n",
    "    dense_layer = tf.keras.layers.Dense(64, activation='relu')(concatenation)\n",
    "\n",
    "    output = tf.keras.layers.Dense(1)(dense_layer)\n",
    "\n",
    "    model = tf.keras.Model(input, output)\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(), \n",
    "                optimizer=keras.optimizers.Adam(lr = learning_rate), \n",
    "                metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(ouput_units = OUTPUT_UNITS, num_units = NUM_UNITS, learning_rate = LEARNING_RATE):\n",
    "     # Generating training sequences\n",
    "     inputs, targets =  training_sequence(LENGTH)\n",
    "\n",
    "     # Neural Network\n",
    "     network = build_dual_network(ouput_units, num_units, learning_rate)\n",
    "\n",
    "\n",
    "     # Training the network\n",
    "     network.fit(inputs, targets, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "     network.save(SAVE_NETWORK_PATH)\n",
    "\n",
    "train()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(output_units, learning_rate):\n",
    "\n",
    "    # Input\n",
    "    input = keras.layers.Input(shape=(None, output_units))\n",
    "\n",
    "    # Bidirectional lstm\n",
    "    bidirectional_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(input)\n",
    "\n",
    "    # Forward lstm\n",
    "    forward_lstm = tf.keras.layers.LSTM(64, return_sequences=True)(bidirectional_lstm)\n",
    "\n",
    "    # Dynamic concatenation between two lstm\n",
    "    concatenation = tf.keras.layers.Concatenate()([bidirectional_lstm, forward_lstm])\n",
    "\n",
    "    # Fully connected layer\n",
    "    dense_layer = tf.keras.layers.Dense(64, activation='relu')(concatenation)\n",
    "\n",
    "    # Output\n",
    "    output = tf.keras.layers.Dense(1)(dense_layer)\n",
    "\n",
    "    model = tf.keras.Model(input, output)\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(), \n",
    "                optimizer=keras.optimizers.Adam(lr = learning_rate), \n",
    "                metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "SAVE_NETWORK_PATH = \"model.h5\"\n",
    "\n",
    "def train(ouput_units = OUTPUT_UNITS, num_units = NUM_UNITS, learning_rate = LEARNING_RATE):\n",
    "     # Generating training sequences\n",
    "     inputs, targets =  training_sequence(LENGTH)\n",
    "\n",
    "     # Neural Network\n",
    "     network = create_model(ouput_units, learning_rate)\n",
    "\n",
    "\n",
    "     # Training the network\n",
    "     network.fit(inputs, targets, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "     network.save(SAVE_NETWORK_PATH)\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27ec4398efe5f1e0762e95a3ab8e2cbf2dfb129f4bce910de35277a628037a44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
