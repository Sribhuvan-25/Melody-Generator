{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3953931785.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    sudo pip uninstall protobuf\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.protobuf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmusic21\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mm21\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.8/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.8/site-packages/tensorflow/python/__init__.py:37\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.8/site-packages/tensorflow/python/eager/context.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabsl\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[1;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m function_pb2\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m config_pb2\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m coordination_config_pb2\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.8/site-packages/tensorflow/core/framework/function_pb2.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      6\u001b[0m _b\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mversion_info[\u001b[39m0\u001b[39m]\u001b[39m<\u001b[39m\u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m (\u001b[39mlambda\u001b[39;00m x:x) \u001b[39mor\u001b[39;00m (\u001b[39mlambda\u001b[39;00m x:x\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m descriptor \u001b[39mas\u001b[39;00m _descriptor\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m message \u001b[39mas\u001b[39;00m _message\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m reflection \u001b[39mas\u001b[39;00m _reflection\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.protobuf'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import music21 as m21\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SONGS_PATH = \"Data/erk\"\n",
    "ACCEPTABLE_DURATIONS = [0.25, 0.5, 0.75, 1.0, 1.5, 2, 3, 4]\n",
    "SAVE = \"Dataset\"\n",
    "SINGLE_FILE_PATH = \"misc/single_file\"\n",
    "LENGTH = 64\n",
    "DICT_PATH = \"misc/dict.json\"\n",
    "\n",
    "def load_songs(datset_path):\n",
    "    songsList = []\n",
    "    for path,subdir,files in os.walk(datset_path):\n",
    "        for file in files:\n",
    "            if file[-3:] == \"krn\":\n",
    "                songsList.append(m21.converter.parse(os.path.join(path, file)))\n",
    "    return songsList\n",
    "\n",
    "def filter_durations(song, ACCEPTABLE_DURATIONS):\n",
    "\n",
    "    for note in song.flat.notesAndRests:\n",
    "        if note.duration.quarterLength not in ACCEPTABLE_DURATIONS:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Changing the music data to time series representation\n",
    "def encode_song(song, timeStamp = 0.25):\n",
    "\n",
    "    encoded_list = []\n",
    "    for event in song.flat.notesAndRests:\n",
    "\n",
    "        if isinstance (event, m21.note.Note):\n",
    "            symbol = event.pitch.midi\n",
    "        \n",
    "        if isinstance(event, m21.note.Rest):\n",
    "            symbol = \"r\"\n",
    "    \n",
    "        steps = int(event.duration.quarterLength / timeStamp)\n",
    "        for step in range(steps):\n",
    "            if step == 0:\n",
    "                encoded_list.append(symbol)\n",
    "            else:\n",
    "                encoded_list.append(\"_\")\n",
    "\n",
    "    encoded_list = \" \".join(map(str, encoded_list))\n",
    "    return encoded_list\n",
    "\n",
    "\n",
    "def transpose(song):\n",
    "\n",
    "    # music score has multiple parts\n",
    "    parts = song.getElementsByClass(m21.stream.Part)\n",
    "    partZero = parts[0].getElementsByClass(m21.stream.Measure)\n",
    "    key = partZero[0][4]\n",
    "\n",
    "    # Estimating key incase there isn't one\n",
    "    if not isinstance(key, m21.key.Key):\n",
    "        key = song.analyze(\"key\")\n",
    "    \n",
    "    # Interval for transposition\n",
    "    if key.mode == \"major\":\n",
    "        interval = m21.interval.Interval(key.tonic, m21.pitch.Pitch(\"C\"))\n",
    "    elif key.mode == 'minor':\n",
    "        interval = m21.interval.Interval(key.tonic, m21.pitch.Pitch(\"A\"))\n",
    "    \n",
    "    transpose_song = song.transpose(interval)\n",
    "    return transpose_song\n",
    "    \n",
    "    \n",
    "def preprocess(dataset_path):\n",
    "    songs = load_songs(dataset_path)\n",
    "    print(f\"# songs loaded = {len(songs)}\")\n",
    "    \n",
    "    for i, song in enumerate(songs):\n",
    "\n",
    "        # Filtering durations\n",
    "        if not filter_durations(song, ACCEPTABLE_DURATIONS):\n",
    "            continue\n",
    "        \n",
    "        # Transpoing to C maj / A min\n",
    "        song = transpose(song)\n",
    "\n",
    "         # Encoding\n",
    "        encoded_song = encode_song(song)\n",
    "\n",
    "        SAVE_songs = os.path.join(SAVE, str(i))\n",
    "        with open(SAVE_songs, \"w\") as fp:\n",
    "            fp.write(encoded_song)\n",
    "    \n",
    "def load(path):\n",
    "    with open(path, \"r\") as fp:\n",
    "        song = fp.read()\n",
    "        return song\n",
    "\n",
    "def create_single_file(dataset_path, file_path, sequence_len):\n",
    "    delimeter = \"/ \" * sequence_len\n",
    "    songs = \"\"\n",
    "\n",
    "# Adding delimeters after loading the encoded songs\n",
    "    for path, subdir, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            filePath = os.path.join(path, file)\n",
    "            song = load(filePath)\n",
    "            songs = songs + song + \" \" + delimeter\n",
    "    # Take everything apart from the last character which is a space\n",
    "    songs = songs[:-1]\n",
    "\n",
    "    with open(file_path, \"w\") as fp:\n",
    "        fp.write(songs)\n",
    "    \n",
    "    return songs\n",
    "\n",
    "def mapping(songs, dict_path):\n",
    "    \n",
    "    dict = {}\n",
    "\n",
    "    songs = songs.split()\n",
    "    uniqueValues = list(set(songs))\n",
    "\n",
    "    for i, el in enumerate(uniqueValues):\n",
    "        dict[el] = i\n",
    "    \n",
    "    with open(dict_path, \"w\") as fp:\n",
    "        json.dump(dict, fp, indent=2)\n",
    "\n",
    "# Converting the song to a list of integers\n",
    "def convert_songs(songs):\n",
    "\n",
    "    intList = []    \n",
    "\n",
    "    # Mapping List\n",
    "    with open(DICT_PATH, \"r\") as fp:\n",
    "        dict = json.load(fp)\n",
    "    \n",
    "    # Converting string to list\n",
    "    songs = songs.split()\n",
    "\n",
    "    # Mapping every symbol in the songs list to int\n",
    "    for el in songs:\n",
    "        intList.append(dict[el])\n",
    "    \n",
    "    return intList\n",
    "\n",
    "# Generating training sequences, which are a subsets of time series music representation\n",
    "def training_sequence(sequence_length):\n",
    "    \n",
    "    songs = load(SINGLE_FILE_PATH)\n",
    "    mapped_songs = convert_songs(songs)\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    # Generating sequences\n",
    "    # Predicting the next musical event in the melody\n",
    "    # 64 time steps for each training sample\n",
    "    total_sequences = len(mapped_songs) - sequence_length\n",
    "    for i in range(total_sequences):\n",
    "        # Ex: [1,2,3,4] [i: [1,2] -> t: [3]] [[2,3] -> [4]] \n",
    "        inputs.append(mapped_songs[i:i+sequence_length])\n",
    "        targets.append(mapped_songs[i+sequence_length])\n",
    "    \n",
    "    # One Hot Ecnoding\n",
    "    # Inputs is a 2D array, (total_sequences, sequence_length, unique_elements)\n",
    "    # [[0,1,2],[1,1,2]] -> [[[1,0,0], [0,1,0], [0,0,1]], []]\n",
    "    #                           0         1        2\n",
    "    unique_elements = len(set(mapped_songs))\n",
    "    inputs = keras.utils.to_categorical(inputs, num_classes=unique_elements)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     preprocess(SONGS_PATH)\n",
    "#     songs = create_single_file(SAVE, SINGLE_FILE_PATH,  LENGTH)\n",
    "#     mapping(songs, DICT_PATH)\n",
    "#     inputs, targets = training_sequence(LENGTH)\n",
    "\n",
    "# main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27ec4398efe5f1e0762e95a3ab8e2cbf2dfb129f4bce910de35277a628037a44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
